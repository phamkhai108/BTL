{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c07300-f70f-4c90-86c7-ab12e9d6141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doan_10.txt', 'doan_6.txt', 0.13146919378291513)\n",
      "('doan_6.txt', 'doan_7.txt', 0.5387950480045754)\n",
      "('doan_10.txt', 'doan_5.txt', 0.10399423210730158)\n",
      "('doan_5.txt', 'doan_8.txt', 0.0)\n",
      "('doan_2.txt', 'doan_3.txt', 0.3696460889854305)\n",
      "('doan_3.txt', 'doan_4.txt', 0.5773247799261676)\n",
      "('doan_1.txt', 'doan_8.txt', 0.0)\n",
      "('doan_3.txt', 'doan_5.txt', 0.006275269898820604)\n",
      "('doan_3.txt', 'doan_6.txt', 0.0)\n",
      "('doan_4.txt', 'doan_7.txt', 0.0041216544735306745)\n",
      "('doan_1.txt', 'doan_7.txt', 0.13235428146127068)\n",
      "('doan_2.txt', 'doan_5.txt', 0.0)\n",
      "('doan_10.txt', 'doan_4.txt', 0.0)\n",
      "('doan_7.txt', 'doan_8.txt', 0.07614751065033604)\n",
      "('doan_5.txt', 'doan_7.txt', 0.22342183689008713)\n",
      "('doan_10.txt', 'doan_2.txt', 0.0)\n",
      "('doan_10.txt', 'doan_7.txt', 0.1468259304865595)\n",
      "('doan_1.txt', 'doan_10.txt', 0.837949330735135)\n",
      "('doan_10.txt', 'doan_3.txt', 0.0)\n",
      "('doan_5.txt', 'doan_6.txt', 0.21443240321227774)\n",
      "('doan_1.txt', 'doan_6.txt', 0.10484779315683229)\n",
      "('doan_4.txt', 'doan_6.txt', 0.0)\n",
      "('doan_1.txt', 'doan_5.txt', 0.10169352722580127)\n",
      "('doan_10.txt', 'doan_8.txt', 0.0)\n",
      "('doan_2.txt', 'doan_6.txt', 0.005137111564287527)\n",
      "('doan_4.txt', 'doan_5.txt', 0.0)\n",
      "('doan_2.txt', 'doan_8.txt', 0.2794774093484151)\n",
      "('doan_1.txt', 'doan_2.txt', 0.0)\n",
      "('doan_6.txt', 'doan_8.txt', 0.026402671065344413)\n",
      "('doan_1.txt', 'doan_3.txt', 0.0)\n",
      "('doan_3.txt', 'doan_7.txt', 0.017299541124743183)\n",
      "('doan_2.txt', 'doan_7.txt', 0.009916403671205564)\n",
      "('doan_3.txt', 'doan_8.txt', 0.5758358847899878)\n",
      "('doan_4.txt', 'doan_8.txt', 0.3342735429970181)\n",
      "('doan_1.txt', 'doan_4.txt', 0.0)\n",
      "('doan_2.txt', 'doan_4.txt', 0.2382408141409662)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "student_files = [doc for doc in os.listdir() if doc.endswith('.txt')]\n",
    "student_notes = [open(_file, encoding='utf-8').read() for _file in student_files]\n",
    "\n",
    "def vectorize(Text): return TfidfVectorizer().fit_transform(Text).toarray()\n",
    "def similarity(doc1, doc2): return cosine_similarity([doc1, doc2])\n",
    "\n",
    "vectors = vectorize(student_notes)\n",
    "s_vectors = list(zip(student_files, vectors))\n",
    "plagiarism_results = set()\n",
    "\n",
    "def check_plagiarism():\n",
    "    global s_vectors\n",
    "    for student_a, text_vector_a in s_vectors:\n",
    "        new_vectors = s_vectors.copy()\n",
    "        current_index = new_vectors.index((student_a, text_vector_a))\n",
    "        del new_vectors[current_index]\n",
    "        for student_b, text_vector_b in new_vectors:\n",
    "            sim_score = similarity(text_vector_a, text_vector_b)[0][1]\n",
    "            student_pair = sorted((student_a, student_b))\n",
    "            score = (student_pair[0], student_pair[1], sim_score)\n",
    "            plagiarism_results.add(score)\n",
    "    return plagiarism_results\n",
    "\n",
    "for data in check_plagiarism():\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaff9dbb-c8d6-47ec-95c2-08b8bcc88ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tương đồng: 0.8585968293130968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def vectorize(Text): \n",
    "    return TfidfVectorizer().fit_transform(Text).toarray()\n",
    "\n",
    "def similarity(doc1, doc2): \n",
    "    return cosine_similarity([doc1, doc2])\n",
    "\n",
    "# Đọc nội dung từ hai tệp tin văn bản\n",
    "document1_text = open(\"doan_1.txt\", encoding='utf-8').read()\n",
    "# document1_text = re.sub(r'[^\\w\\s]', '', document1_text)\n",
    "# document1_text = re.sub(r'[^\\w\\s]', '', unidecode(document1_text))\n",
    "document2_text = open(\"doan_10.txt\", encoding='utf-8').read()\n",
    "\n",
    "# Chuyển đổi đoạn văn thành vector\n",
    "vectors = vectorize([document1_text, document2_text])\n",
    "\n",
    "# So sánh tương đồng giữa hai đoạn văn\n",
    "similarity_score = similarity(vectors[0], vectors[1])[0][1]\n",
    "\n",
    "print(\"Tương đồng:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a399da3-f2d3-4494-9b65-e34a6702bbf1",
   "metadata": {},
   "source": [
    "# Test với url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abfbc30-1478-4b53-a271-870cdd81c2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Ignoring args : ('The development of artificial intelligence (AI) has witnessed tremendous progress in recent years. From chatbots to autonomous vehicles, AI applications have become an integral part of our daily lives. Machine learning algorithms, a subset of AI, play a crucial role in enabling computers to learn from data and make decisions without explicit programming.\\n\\nOne area where AI has made significant strides is natural language processing (NLP). NLP algorithms allow machines to understand, interpret, and generate human-like language. This has led to advancements in virtual assistants, language translation, and sentiment analysis.\\n\\nDespite the numerous benefits of AI, ethical considerations and concerns about job displacement have emerged. Ensuring the responsible and fair use of AI technologies is a challenge that requires collaboration between technologists, policymakers, and ethicists. As AI continues to evolve, addressing these ethical dilemmas will be crucial for building a positive and sustainable future.\\n\\nIn conclusion, artificial intelligence has transformed the way we interact with technology, bringing about new possibilities and challenges. As we navigate this rapidly evolving landscape, it is essential to prioritize ethical considerations and ensure that AI benefits humanity as a whole.\\n',)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2629 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2629) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\KIKIp\\Documents\\Xử Lý Ngôn Ngữ Tự Nhiên\\test_tom_tat.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m document2_text \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdoan_2.txt\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Kiểm tra đạo văn\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m is_plagiarized \u001b[39m=\u001b[39m check_plagiarism_with_tfidf_bert(document1_text, document2_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIs Plagiarized:\u001b[39m\u001b[39m\"\u001b[39m, is_plagiarized)\n",
      "\u001b[1;32mc:\\Users\\KIKIp\\Documents\\Xử Lý Ngôn Ngữ Tự Nhiên\\test_tom_tat.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m similarity_score_tfidf \u001b[39m=\u001b[39m similarity(vectors_tfidf[\u001b[39m0\u001b[39m], vectors_tfidf[\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Tính toán độ tương đồng với DistilBERT\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m similarity_score_bert \u001b[39m=\u001b[39m calculate_similarity_score_bert(text1, text2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTương đồng TF-IDF:\u001b[39m\u001b[39m\"\u001b[39m, similarity_score_tfidf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# print(\"Tương đồng DistilBERT:\", similarity_score_bert)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Kết hợp kết quả từ cả hai phương pháp\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\KIKIp\\Documents\\Xử Lý Ngôn Ngữ Tự Nhiên\\test_tom_tat.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m similarity_analyzer \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39mtext-classification\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Tính toán độ tương đồng\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m similarity_score \u001b[39m=\u001b[39m similarity_analyzer(text1, text2)[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/KIKIp/Documents/X%E1%BB%AD%20L%C3%BD%20Ng%C3%B4n%20Ng%E1%BB%AF%20T%E1%BB%B1%20Nhi%C3%AAn/test_tom_tat.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m similarity_score\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    157\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:779\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    777\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 779\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[0;32m    780\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    781\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    782\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    783\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    784\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    785\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    786\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    787\u001b[0m )\n\u001b[0;32m    788\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    789\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:597\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    595\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 597\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[0;32m    600\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[0;32m    601\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    605\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KIKIp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:135\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[1;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[0;32m    131\u001b[0m     position_ids \u001b[39m=\u001b[39m position_ids\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mexpand_as(input_ids)  \u001b[39m# (bs, max_seq_length)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m embeddings \u001b[39m=\u001b[39m input_embeds \u001b[39m+\u001b[39;49m position_embeddings  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    136\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    137\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2629) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, pipeline\n",
    "\n",
    "def vectorize(Text): \n",
    "    return TfidfVectorizer().fit_transform(Text).toarray()\n",
    "\n",
    "def similarity(doc1, doc2): \n",
    "    return cosine_similarity([doc1, doc2])\n",
    "\n",
    "def calculate_similarity_score_bert(text1, text2):\n",
    "    # Sử dụng DistilBERT để tính toán độ tương đồng giữa hai đoạn văn\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=1)\n",
    "    similarity_analyzer = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Tính toán độ tương đồng\n",
    "    similarity_score = similarity_analyzer(text1, text2)[0]['score']\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "def check_plagiarism_with_tfidf_bert(text1, text2, threshold_tfidf=0.8, threshold_bert=0.8):\n",
    "    # Vector hóa dữ liệu bằng TF-IDF\n",
    "    vectors_tfidf = vectorize([text1, text2])\n",
    "\n",
    "    # Tính toán độ tương đồng với TF-IDF\n",
    "    similarity_score_tfidf = similarity(vectors_tfidf[0], vectors_tfidf[1])[0][1]\n",
    "\n",
    "    # Tính toán độ tương đồng với DistilBERT\n",
    "    similarity_score_bert = calculate_similarity_score_bert(text1, text2)\n",
    "\n",
    "    print(\"Tương đồng TF-IDF:\", similarity_score_tfidf)\n",
    "    # print(\"Tương đồng DistilBERT:\", similarity_score_bert)\n",
    "\n",
    "    # Kết hợp kết quả từ cả hai phương pháp\n",
    "    combined_score = (similarity_score_tfidf + similarity_score_bert) / 2\n",
    "\n",
    "    return combined_score >= max(threshold_tfidf, threshold_bert)\n",
    "\n",
    "# Đọc nội dung từ hai tệp tin văn bản\n",
    "document1_text = open(\"doan_1.txt\", encoding='utf-8').read()\n",
    "document2_text = open(\"doan_10.txt\", encoding='utf-8').read()\n",
    "\n",
    "# Kiểm tra đạo văn\n",
    "is_plagiarized = check_plagiarism_with_tfidf_bert(document1_text, document2_text)\n",
    "print(\"Is Plagiarized:\", is_plagiarized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test thư viện dịch của google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiện, việc bắn pháo hoa thực hiện theo Nghị định 362009. Vào giao thừa Tết Nguyên đán, các thành phố trực thuộc Trung ương và Thừa Thiên Huế được bắn pháo hoa nổ tầm cao và tầm thấp, thời lượng không quá 15 phút. Các tỉnh còn lại được bắn pháo hoa nổ tầm thấp không quá 15 phút.\n",
      "Ngày Quốc khánh, các thành phố trực thuộc Trung ương và Thừa Thiên Huế được bắn pháo hoa nổ tầm cao và tầm thấp vào lúc 21h ngày 29 trong 15 phút.\n",
      "Ngày 304, Hà Nội và TP HCM được bắn pháo hoa nổ tầm cao và tầm thấp không quá 15 phút, vào 21h.\n",
      "Ngoài ra các dịp được bắn pháo hoa như ngày thành lập các tỉnh, thành phố thuộc Trung ương sự kiện văn hóa, du lịch, thể thao mang tính quốc gia, quốc tế... Những trường hợp khác, trong đó có Tết Dương lịch, do Thủ tướng quyết định.\n",
      "Lê Tuyết\n",
      "Currently, fireworks displays are carried out in accordance with Decree 362009. On Lunar New Year's Eve, centrally run cities and Thua Thien Hue are allowed to display high- and low-range fireworks, lasting no more than 15 minutes. The remaining provinces are allowed to display low-level fireworks for no more than 15 minutes.\n",
      "On National Day, centrally run cities and Thua Thien Hue can display fireworks at high and low altitudes at 9:00 p.m. on the 29th for 15 minutes.\n",
      "On Day 304, Hanoi and Ho Chi Minh City were allowed to display fireworks at high and low altitudes for no more than 15 minutes, at 9:00 p.m.\n",
      "In addition, there are occasions when fireworks are displayed such as the founding days of provinces and centrally run cities, national and international cultural, tourist and sports events... Other cases, including New Year's Eve schedule, decided by the Prime Minister.\n",
      "Le Tuyet\n"
     ]
    }
   ],
   "source": [
    "# Import thư viện deep-translator\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Định nghĩa một đoạn văn bản tiếng Việt\n",
    "# Định nghĩa tên file và đường dẫn\n",
    "filename = \"doan_1.txt\"\n",
    "# filepath = \"c:/users/user/documents/\" + filename\n",
    "\n",
    "# Mở file ở chế độ đọc\n",
    "file = open(filename, \"r\")\n",
    "\n",
    "# Đọc nội dung file và lưu vào biến text\n",
    "text = file.read()\n",
    "\n",
    "# Đóng file\n",
    "file.close()\n",
    "\n",
    "# In nội dung file\n",
    "print(text)\n",
    "# Dịch đoạn văn bản sang tiếng Anh bằng GoogleTranslator\n",
    "translation = GoogleTranslator(source='vi', target='en').translate(text)\n",
    "\n",
    "# In kết quả dịch\n",
    "print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thư viện deep-translator\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Nhập vào đoạn văn bản ngôn ngữ bất kì\n",
    "text = input(\"Nhập vào đoạn văn bản ngôn ngữ bất kì: \")\n",
    "\n",
    "# Xác định ngôn ngữ của đoạn văn bản bằng hàm detect_language\n",
    "source = GoogleTranslator.detect_language(text)\n",
    "\n",
    "# Dịch đoạn văn bản sang tiếng Anh bằng hàm translate\n",
    "translation = GoogleTranslator(source=source, target='en').translate(text)\n",
    "\n",
    "# In kết quả dịch\n",
    "print(\"Đoạn văn bản đã được dịch sang tiếng Anh là: \")\n",
    "print(translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lấy dữ liệu tưg trsng web và lưu vào tệp txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nội dung đã được lưu vào file doan_10.txt.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from underthesea import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL của trang web\n",
    "url = \"https://vi.wikipedia.org/wiki/T%C6%B0%E1%BB%A3ng_N%E1%BB%AF_th%E1%BA%A7n_T%E1%BB%B1_do\"\n",
    "\n",
    "# Sử dụng requests để tải nội dung trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem việc tải nội dung thành công hay không\n",
    "if response.status_code == 200:\n",
    "    # Lấy nội dung HTML của trang web\n",
    "    html = response.text\n",
    "\n",
    "    # Tạo một đối tượng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Trích xuất văn bản từ trang web và loại bỏ các đoạn trống\n",
    "    text = '\\n'.join([line.strip() for line in soup.stripped_strings])\n",
    "\n",
    "    # Lưu nội dung vào file\n",
    "    with open(\"doan_10.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(\"Nội dung đã được lưu vào file doan_10.txt.\")\n",
    "else:\n",
    "    print(f\"Không thể tải nội dung. Mã trạng thái: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chỉ lấy văn bản trong thẻ p. bỏ kí tự đặt biệt giữ lại câu tiếng việt và , ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nội dung đã được lưu vào file doan_10.txt.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL của trang web\n",
    "url = \"https://vnexpress.net/tp-hcm-xin-ban-phao-hoa-dip-tet-duong-lich-4682385.html\"\n",
    "\n",
    "# Sử dụng requests để tải nội dung trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem việc tải nội dung thành công hay không\n",
    "if response.status_code == 200:\n",
    "    # Lấy nội dung HTML của trang web\n",
    "    html = response.text\n",
    "\n",
    "    # Tạo một đối tượng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Tìm tất cả các thẻ <p> và lấy nội dung từ chúng\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    # Hàm loại bỏ kí tự đặc biệt và giữ lại dấu câu\n",
    "    def remove_special_characters(text):\n",
    "        cleaned_text = re.sub(r'[^\\w\\s.,?!]', '', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Lấy văn bản từ các thẻ <p> và loại bỏ các đoạn trống\n",
    "    text = '\\n'.join([remove_special_characters(paragraph.get_text()).strip() for paragraph in paragraphs])\n",
    "\n",
    "    # Lưu nội dung vào file\n",
    "    with open(\"doan_10.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(\"Nội dung đã được lưu vào file doan_10.txt.\")\n",
    "else:\n",
    "    print(f\"Không thể tải nội dung. Mã trạng thái: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lấy vưn bản thẻ p trong web rồi loại bỏ kí tự đặt biệt. Cả đấu chấm và dấu phẩy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nội dung đã được lưu vào file doan_10.txt.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL của trang web\n",
    "url = \"https://vi.wikipedia.org/wiki/T%C6%B0%E1%BB%A3ng_N%E1%BB%AF_th%E1%BA%A7n_T%E1%BB%B1_do\"\n",
    "\n",
    "# Sử dụng requests để tải nội dung trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem việc tải nội dung thành công hay không\n",
    "if response.status_code == 200:\n",
    "    # Lấy nội dung HTML của trang web\n",
    "    html = response.text\n",
    "\n",
    "    # Tạo một đối tượng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Tìm tất cả các thẻ <p> và lấy nội dung từ chúng\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    # Hàm để loại bỏ kí tự đặc biệt, dấu câu và khoảng trắng không mong muốn\n",
    "    def clean_text(text):\n",
    "        # Loại bỏ kí tự đặc biệt, dấu câu và chuyển đổi thành chữ thường\n",
    "        cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Loại bỏ dấu câu và chuyển đổi thành chữ thường\n",
    "        cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    # Lấy văn bản từ các thẻ <p>, loại bỏ kí tự đặc biệt, dấu câu và khoảng trắng không mong muốn\n",
    "    text = '\\n'.join([clean_text(paragraph.get_text()) for paragraph in paragraphs])\n",
    "\n",
    "    # Lưu nội dung vào file\n",
    "    with open(\"doan_10.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(\"Nội dung đã được lưu vào file doan_10.txt.\")\n",
    "else:\n",
    "    print(f\"Không thể tải nội dung. Mã trạng thái: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nội dung đã được lưu vào file doan_10.txt.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "# URL của trang web\n",
    "url = \"https://vi.wikipedia.org/wiki/T%C6%B0%E1%BB%A3ng_N%E1%BB%AF_th%E1%BA%A7n_T%E1%BB%B1_do\"\n",
    "\n",
    "# Sử dụng requests để tải nội dung trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem việc tải nội dung thành công hay không\n",
    "if response.status_code == 200:\n",
    "    # Lấy nội dung HTML của trang web\n",
    "    html = response.text\n",
    "\n",
    "    # Tạo một đối tượng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Tìm tất cả các thẻ <p> và lấy nội dung từ chúng\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    # Hàm để loại bỏ kí tự đặc biệt, dấu câu và khoảng trắng không mong muốn, và xóa dấu tiếng Việt\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        cleaned_text = re.sub(r'[^\\w\\s]', '', unidecode(cleaned_text))\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    # Lấy văn bản từ các thẻ <p>, loại bỏ kí tự đặc biệt, dấu câu, khoảng trắng không mong muốn, và xóa dấu tiếng Việt\n",
    "    text = '\\n'.join([clean_text(paragraph.get_text()) for paragraph in paragraphs])\n",
    "\n",
    "    # Lưu nội dung vào file\n",
    "    with open(\"doan_10.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(\"Nội dung đã được lưu vào file doan_10.txt.\")\n",
    "else:\n",
    "    print(f\"Không thể tải nội dung. Mã trạng thái: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nội dung đã được lưu vào file doan_10.txt.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "# URL của trang web\n",
    "url = \"https://vnexpress.net/\"\n",
    "\n",
    "# Sử dụng requests để tải nội dung trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem việc tải nội dung thành công hay không\n",
    "if response.status_code == 200:\n",
    "    # Lấy nội dung HTML của trang web\n",
    "    html = response.text\n",
    "\n",
    "    # Tạo một đối tượng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Tìm tất cả các thẻ <p> và lấy nội dung từ chúng\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    # Hàm để chỉ loại bỏ kí tự đặc biệt và xóa dấu tiếng Việt\n",
    "    def clean_text(text):\n",
    "        cleaned_text = re.sub(r'[^\\w\\s.,?!]', '', unidecode(text))\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    # Lấy văn bản từ các thẻ <p>, loại bỏ kí tự đặc biệt, và xóa dấu tiếng Việt\n",
    "    text = '\\n'.join([clean_text(paragraph.get_text()) for paragraph in paragraphs])\n",
    "\n",
    "    # Lưu nội dung vào file\n",
    "    with open(\"doan_10.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(\"Nội dung đã được lưu vào file doan_10.txt.\")\n",
    "else:\n",
    "    print(f\"Không thể tải nội dung. Mã trạng thái: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
