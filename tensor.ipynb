{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'bắn': 2, 'và': 3, 'pháo': 4, 'hoa': 5, 'tầm': 6, '15': 7, 'phút': 8, 'được': 9, 'lịch': 10, 'thành': 11, 'hóa': 12, 'tết': 13, 'thấp': 14, 'ngày': 15, 'các': 16, 'phố': 17, 'tại': 18, 'văn': 19, 'tp': 20, 'dịp': 21, 'dương': 22, 'cao': 23, 'vào': 24, 'nổ': 25, 'sông': 26, 'sài': 27, 'gòn': 28, 'hcm': 29, 'du': 30, 'thủ': 31, 'thừa': 32, 'thuộc': 33, 'trung': 34, 'ương': 35, 'không': 36, 'quá': 37, 'quốc': 38, 'nghị': 39, 'đầu': 40, 'đường': 41, 'hầm': 42, 'công': 43, 'viên': 44, 'đầm': 45, 'sen': 46, 'nội': 47, 'thể': 48, 'thao': 49, 'tướng': 50, 'theo': 51, 'đó': 52, 'thời': 53, '304': 54, '29': 55, 'nguyên': 56, 'đán': 57, 'hiện': 58, 'định': 59, 'trực': 60, 'thiên': 61, 'huế': 62, 'tỉnh': 63, '21h': 64, 'trong': 65, 'chính': 66, 'quyền': 67, 'kiến': 68, 'mừng': 69, 'năm': 70, 'mới': 71, 'dung': 72, 'vừa': 73, 'ubnd': 74, 'gửi': 75, 'bộ': 76, 'để': 77, 'trình': 78, 'chấp': 79, 'thuận': 80, 'cho': 81, '2024': 82, 'rực': 83, 'sáng': 84, 'trên': 85, '2023': 86, 'ảnh': 87, 'quỳnh': 88, 'trần': 89, 'điểm': 90, 'khu': 91, 'vực': 92, 'đức': 93, 'quận': 94, '11': 95, 'gian': 96, 'từ': 97, '0h': 98, 'đến': 99, '0h15': 100, '112024': 101, 'nguồn': 102, 'kinh': 103, 'phí': 104, 'xã': 105, 'hội': 106, 'là': 107, 'hoạt': 108, 'động': 109, 'thường': 110, 'xuyên': 111, 'lễ': 112, 'lớn': 113, 'phục': 114, 'vụ': 115, 'người': 116, 'dân': 117, 'khách': 118, 'trừ': 119, 'giai': 120, 'đoạn': 121, 'covid19': 122, 'bùng': 123, 'phát': 124, 'việc': 125, 'thực': 126, '362009': 127, 'giao': 128, 'lượng': 129, 'còn': 130, 'lại': 131, 'khánh': 132, 'lúc': 133, 'hà': 134, 'ngoài': 135, 'ra': 136, 'như': 137, 'lập': 138, 'sự': 139, 'kiện': 140, 'mang': 141, 'tính': 142, 'gia': 143, 'tế': 144, 'những': 145, 'trường': 146, 'hợp': 147, 'khác': 148, 'có': 149, 'do': 150, 'quyết': 151, 'lê': 152, 'tuyết': 153}\n",
      "\n",
      "Sequences =  []\n",
      "\n",
      "Padded Sequences:\n",
      "[]\n",
      "\n",
      "Test Sequence =  [[6, 13, 1, 9, 1], [6, 4]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[ 0  0  0  0  0  6 13  1  9  1]\n",
      " [ 0  0  0  0  0  0  0  0  6  4]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = open(\"doan_10.txt\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)\n",
    "\n",
    "\n",
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'tầm tết trọng được gì',\n",
    "    'Tầm Pháo'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2990839e-03 -9.8079853e-03  4.5898892e-03 -5.4324645e-04\n",
      "  6.3352021e-03  1.7859326e-03 -3.1301705e-03  7.7640968e-03\n",
      "  1.5586240e-03  5.5723962e-05 -4.6120691e-03 -8.4542688e-03\n",
      " -7.7717816e-03  8.6704874e-03 -8.9298133e-03  9.0338066e-03\n",
      " -9.2804348e-03 -2.7806018e-04 -1.9105632e-03 -8.9337146e-03\n",
      "  8.6312890e-03  6.7778910e-03  3.0246512e-03  4.8347209e-03\n",
      "  1.0861163e-04  9.4263339e-03  7.0256954e-03 -9.8567912e-03\n",
      " -4.4366326e-03 -1.2886900e-03  3.0495788e-03 -4.3272525e-03\n",
      "  1.4508392e-03 -7.8469114e-03  2.7776337e-03  4.7015138e-03\n",
      "  4.9363803e-03 -3.1773932e-03 -8.4278435e-03 -9.2248805e-03\n",
      " -7.2485313e-04 -7.3265689e-03 -6.8137795e-03  6.1227162e-03\n",
      "  7.1754926e-03  2.1180201e-03 -7.9002595e-03 -5.7018576e-03\n",
      "  8.0538243e-03  3.9243493e-03 -5.2433652e-03 -7.3936800e-03\n",
      "  7.7276141e-04  3.4599975e-03  2.0783425e-03  3.0983554e-03\n",
      " -5.6210137e-03 -9.8885428e-03 -7.0211114e-03  2.3111385e-04\n",
      "  4.6216170e-03  4.5280904e-03  1.8817896e-03  5.1702997e-03\n",
      " -1.0753617e-04  4.1136066e-03 -9.1232853e-03  7.7048945e-03\n",
      "  6.1487914e-03  5.1226602e-03  7.2126850e-03  8.4420461e-03\n",
      "  7.3977758e-04 -1.7018793e-03  5.2066933e-04 -9.3221841e-03\n",
      "  8.4112780e-03 -6.3818041e-03  8.4274197e-03 -4.2477190e-03\n",
      "  6.4723263e-04 -9.1661727e-03 -9.5639564e-03 -7.8350781e-03\n",
      " -7.7333236e-03  3.7358521e-04 -7.2277570e-03 -4.9520470e-03\n",
      " -5.2769077e-03 -4.2920639e-03  7.0131165e-03  4.8308093e-03\n",
      "  8.6888326e-03  7.0931637e-03 -5.6947609e-03  7.2457488e-03\n",
      " -9.2952885e-03 -2.5886064e-03 -7.7588656e-03  4.1914661e-03]\n"
     ]
    }
   ],
   "source": [
    "# Nhập các mô-đun nltk và gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Định nghĩa một đoạn văn tiếng Việt\n",
    "paragraph = \"Tôi là Bing, một công cụ tìm kiếm trực tuyến của Microsoft. Tôi có thể giúp bạn tìm kiếm thông tin, hình ảnh, tin tức, video và nhiều thứ khác trên mạng. Tôi cũng có thể tạo ra nội dung sáng tạo và độc đáo như thơ, truyện, mã, bài luận, bài hát, chế nhạo người nổi tiếng và nhiều hơn nữa bằng lời và kiến thức của riêng tôi.\"\n",
    "\n",
    "# Tách đoạn văn thành các câu bằng hàm sent_tokenize\n",
    "sentences = sent_tokenize (paragraph)\n",
    "\n",
    "# Tách mỗi câu thành các từ bằng hàm word_tokenize\n",
    "words = [word_tokenize (sentence) for sentence in sentences]\n",
    "\n",
    "# Tạo một mô hình Word2Vec với các tham số như kích thước vector, số lần xuất hiện tối thiểu, cửa sổ ngữ cảnh và kiến trúc\n",
    "model = Word2Vec (words, vector_size=100, min_count=1, window=5, sg=1)\n",
    "\n",
    "# In vector của từ \"Bing\" bằng cách truy cập thuộc tính wv\n",
    "print(model.wv [\"Bing\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'tôi': 2, 'bạn': 3, 'là': 4, 'không': 5, 'đi': 6, 'chơi': 7, 'à': 8, 'siêu': 9, 'nhân': 10, 'yêu': 11, 'thú': 12, 'cưng': 13, 'nghĩ': 14, 'ai': 15}\n",
      "\n",
      "Sequences =  [[3, 5, 6, 7, 8], [2, 4, 9, 10], [2, 11, 12, 13], [3, 14, 2, 4, 15]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 3  5  6  7  8]\n",
      " [ 0  2  4  9 10]\n",
      " [ 0  2 11 12 13]\n",
      " [ 3 14  2  4 15]]\n",
      "\n",
      "Test Sequence =  [[1, 1, 1, 1, 1], [2, 14, 3, 1, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[ 0  0  0  0  0  1  1  1  1  1]\n",
      " [ 0  0  0  0  0  2 14  3  1  1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'Bạn không đi chơi à',\n",
    "    'tôi Là siêu nhân',\n",
    "    'Tôi yêu thú cưng!',\n",
    "    'bạn nghĩ tôi là ai'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)\n",
    "\n",
    "\n",
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'tôi nghĩ bạn bị điên'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'pháp': 2, 'laboulaye': 3, 'với': 4, 'tượng': 5, 'là': 6, 'trong': 7, 'chính': 8, 'cũng': 9, 'hội': 10, 'giới': 11, 'có': 12, 'chiến': 13, 'thì': 14, 'kỳ': 15, 'động': 16, 'và': 17, 'cuộc': 18, 'năm': 19, 'hoa': 20, 'nhân': 21, 'vận': 22, 'hậu': 23, 'để': 24, 'thực': 25, 'dự': 26, 'án': 27, 'cho': 28, '32': 29, 'còn': 30, 'xây': 31, 'của': 32, 'ở': 33, 'dù': 34, 'công': 35, 'người': 36, 'sĩ': 37, 'tiếng': 38, 'ra': 39, 'kịch': 40, 'được': 41, 'la': 42, 'nhưng': 43, 'đồng': 44, '000': 45, 'trị': 46, 'sau': 47, 'tranh': 48, 'phổ': 49, 'dân': 50, 'thế': 51, '1876': 52, 'tại': 53, 'này': 54, 'mới': 55, 'thuẫn': 56, 'hiện': 57, 'tháng': 58, 'ông': 59, 'thông': 60, 'báo': 61, 'lập': 62, 'liên': 63, 'mỹpháp': 64, 'gây': 65, 'kế': 66, 'hoạch': 67, 'trợ': 68, 'đúc': 69, 'mỹ': 70, 'phần': 71, 'trải': 72, 'khởi': 73, 'vẫn': 74, 'đã': 75, 'phe': 76, 'đối': 77, 'bức': 78, 'nghị': 79, 'cùng': 80, 'thân': 81, 'buổi': 82, 'nhạc': 83, 'ngày': 84, 'hoàn': 85, 'tuy': 86, 'thành': 87, 'xã': 88, 'tham': 89, 'góp': 90, 'tiền': 91, 'toàn': 92, 'lượng': 93, 'đô': 94, 'đương': 95, 'nay': 96, 'chưa': 97, 'khi': 98, 'tình': 99, 'hình': 100, 'dần': 101, 'ổn': 102, 'định': 103, 'kinh': 104, 'tế': 105, 'hồi': 106, 'phục': 107, 'chúng': 108, 'náo': 109, 'nức': 110, 'mong': 111, 'đợi': 112, 'chợ': 113, 'sắp': 114, 'khai': 115, 'trương': 116, 'philadelphia': 117, 'cơ': 118, 'tìm': 119, '31': 120, '9': 121, '1875': 122, 'thức': 123, 'vai': 124, 'trò': 125, 'quỹ': 126, 'nữ': 127, 'thần': 128, 'tự': 129, 'đo': 130, 'soi': 131, 'sáng': 132, 'nhận': 133, 'tài': 134, 'việc': 135, 'sẽ': 136, 'lãnh': 137, 'trang': 138, 'khoản': 139, 'phí': 140, 'bệ': 141, '33': 142, 'nhiều': 143, 'phấn': 144, 'luận': 145, 'oán': 146, 'giận': 147, 'không': 148, 'hỗ': 149, 'vừa': 150, 'qua': 151, 'bảo': 152, 'hoàng': 153, 'phản': 154, 'chỉ': 155, 'vì': 156, 'xướng': 157, 'mà': 158, 'thuộc': 159, 'cấp': 160, 'tiến': 161, 'vậy': 162, 'lại': 163, 'đắc': 164, 'cử': 165, 'một': 166, '75': 167, 'nhiệm': 168, 'trọn': 169, 'đời33': 170, 'sénateur': 171, 'inamovible': 172, 'bất': 173, 'khả': 174, 'phế': 175, 'nên': 176, 'tăm': 177, 'càng': 178, 'nổi': 179, 'liền': 180, 'sức': 181, 'hào': 182, 'bằng': 183, 'trình': 184, 'diễn': 185, 'đặc': 186, 'biệt': 187, 'nhà': 188, 'hát': 189, 'paris': 190, 'vở': 191, '25': 192, '3': 193, 'mắt': 194, 'cantata': 195, 'charles': 196, 'gounod': 197, 'tất': 198, 'tựa': 199, 'liberté': 200, 'éclairant': 201, 'le': 202, 'monde': 203, 'đó': 204, 'tên': 205, 'ban': 206, 'đầu': 207, 'họ': 208, 'tập': 209, 'trung': 210, 'thượng': 211, 'lưu': 212, 'huy': 213, 'mọi': 214, 'tầng': 215, 'lớp': 216, 'thường': 217, 'cả': 218, 'học': 219, 'sinh': 220, 'đều': 221, 'gia': 222, '181': 223, 'rộng': 224, 'khắp': 225, 'nước': 226, 'hưởng': 227, 'ứng': 228, 'lời': 229, 'kêu': 230, 'gọi': 231, 'nồng': 232, 'nhiệt': 233, 'hữu': 234, 'duệ': 235, 'quân': 236, 'từng': 237, 'cách': 238, 'mạng': 239, 'ngoài': 240, 'những': 241, 'dụng': 242, 'ý': 243, 'muốn': 244, 'mua': 245, 'chuộc': 246, 'kênh': 247, 'đào': 248, 'panama': 249, 'ty': 250, 'japy': 251, 'frères': 252, 'chuyên': 253, 'buôn': 254, 'hiến': 255, 'tặng': 256, 'cần': 257, 'thiết': 258, 'giá': 259, '64': 260, 'franc': 261, 'khoảng': 262, '16': 263, 'thời': 264, 'tương': 265, '323': 266, '3435': 267, 'rằng': 268, 'xuất': 269, 'xứ': 270, 'từ': 271, 'visnes': 272, 'na': 273, 'uy': 274, '36': 275, 'đến': 276, 'rõ': 277, 'hư': 278, '37': 279, 'dựng': 280, 'kết': 281}\n",
      "\n",
      "Sequences =  [[7], [98], [99, 1], [8, 46], [2], [1], [1, 1], [17], [1, 1], [9], [1, 1], [47], [18], [13, 48], [4], [49], [14], [50, 1], [9], [1, 1], [1, 1], [10, 1], [51, 11], [19], [52], [1], [1, 1], [53], [1], [], [20, 15], [], [21], [1, 10], [54], [3], [55], [22, 16], [1], [23, 56], [24], [25, 57], [26, 27], [], [1], [58], [1], [19], [1], [], [59], [8, 1], [60, 61], [26, 27], [17], [62], [63, 10], [64], [4], [1, 1], [65], [1], [28], [66, 67], [1, 1], [1, 1], [1, 1], [51, 11], [], [29], [2], [1], [1, 68], [1], [69, 5], [30], [70], [14], [1], [1, 71], [1, 72], [1], [1], [31], [1], [5], [], [1], [60, 61], [26, 27], [32], [3], [65], [1], [1, 73], [33], [2], [28, 34], [35, 1], [33], [2], [74], [30], [12], [36], [1], [1, 20, 15], [75], [1], [1, 68], [2], [7], [18, 13], [4], [49], [1, 1], [], [29], [76], [1, 1], [33], [2], [14], [1, 77], [78], [5], [1], [1], [3], [6], [36], [73, 1], [1], [3], [14], [1], [76], [1, 1], [77, 62], [], [34, 1], [3], [1], [1, 1], [6], [1], [7], [1], [79, 37], [4], [1, 15], [1], [1], [38], [2], [1, 1], [], [79, 37], [1, 1, 1], [1], [38, 1], [59], [1], [1], [], [3], [1], [39, 1], [22, 16], [8, 11], [2], [80], [81, 1], [21, 37], [1], [82], [1, 1], [1, 1], [53], [1, 1], [83, 40], [1], [], [1], [40], [84], [1], [58], [1], [19], [52], [9], [6], [82], [39, 1], [83, 40], [1], [55], [41], [1, 1], [85, 1], [4], [1], [6], [42, 1], [1, 1], [1], [], [1], [9], [8], [6], [1], [38], [2], [32], [78], [5], [], [29], [86], [1, 1], [1], [1, 1], [22, 16], [11], [1, 1], [43], [47], [63, 10], [64], [75], [87, 35], [1, 16], [41], [1], [1, 1], [88, 10], [1, 50], [17], [1], [1, 1], [1], [89, 1], [], [12], [1], [88], [72], [1], [1], [1], [2], [9], [90], [91], [], [1, 1], [1], [1, 1], [1, 1], [6], [81, 1], [32], [3], [7], [8, 11], [2], [80], [23, 1], [1, 21], [2], [1], [89, 13], [7], [18], [13, 48], [1, 1], [70], [], [1, 39], [30], [12], [1], [87, 71], [86], [90], [91], [43], [12, 1], [1, 1], [1, 1], [20, 15], [24], [23, 56], [2], [25, 57], [1], [1], [1], [], [35, 1], [1, 1], [], [1], [1], [44, 1], [1], [92], [93], [44], [1, 1], [24], [31], [5], [], [46, 1], [1, 45, 1], [1], [1, 45], [94, 42], [95, 1], [], [1, 95], [4], [1, 45], [94, 42], [84, 96], [], [1], [12], [36], [28], [1], [93], [44], [54], [1, 1], [1], [1], [], [1, 1], [], [1], [43], [1], [96], [74], [97], [1], [1, 25], [], [1], [34], [66, 67], [31, 1], [5], [97], [41], [85, 92], [69, 1]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0  0  0  0  7]\n",
      " [ 0  0  0  0 98]\n",
      " [ 0  0  0 99  1]\n",
      " ...\n",
      " [ 0  0  0  0 41]\n",
      " [ 0  0  0 85 92]\n",
      " [ 0  0  0 69  1]]\n",
      "\n",
      "Test Sequence =  [[1, 1], [6], [1], [1], [35, 1], [33], [2]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[ 0  0  0  0  0  0  0  0  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0 35  1]\n",
      " [ 0  0  0  0  0  0  0  0  0 33]\n",
      " [ 0  0  0  0  0  0  0  0  0  2]]\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#lấy văn bản từ file\n",
    "text = open(\"doan_1.txt\", encoding='utf-8').read()\n",
    "\n",
    "# Tách từ\n",
    "words = word_tokenize(text)\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(words)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(words)\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)\n",
    "\n",
    "\n",
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = 'quan trọng là tấm lòng công cộng ở pháp'\n",
    "\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(word_tokenize(test_data))\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "['Xin', 'chào', ',', 'tôi', 'Phạm Hồng Khải', ',', 'Rất', 'vui', 'được', 'gặp', 'bạn', '.']\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize, sent_tokenize,sentiment\n",
    "\n",
    "# Chuỗi cần tách từ\n",
    "text = \"Xin chào, tôi Phạm Hồng Khải, Rất vui được gặp bạn.\"\n",
    "a = sentiment(text)\n",
    "# Sử dụng hàm word_tokenize để tách từ\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# In kết quả\n",
    "print(a)\n",
    "print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
