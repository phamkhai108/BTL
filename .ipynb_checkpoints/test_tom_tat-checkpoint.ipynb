{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c07300-f70f-4c90-86c7-ab12e9d6141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('doan_1.txt', 'doan_2.txt', 0.7479716320477551)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "student_files = [doc for doc in os.listdir() if doc.endswith('.txt')]\n",
    "student_notes = [open(_file, encoding='utf-8').read() for _file in student_files]\n",
    "\n",
    "def vectorize(Text): return TfidfVectorizer().fit_transform(Text).toarray()\n",
    "def similarity(doc1, doc2): return cosine_similarity([doc1, doc2])\n",
    "\n",
    "vectors = vectorize(student_notes)\n",
    "s_vectors = list(zip(student_files, vectors))\n",
    "plagiarism_results = set()\n",
    "\n",
    "def check_plagiarism():\n",
    "    global s_vectors\n",
    "    for student_a, text_vector_a in s_vectors:\n",
    "        new_vectors = s_vectors.copy()\n",
    "        current_index = new_vectors.index((student_a, text_vector_a))\n",
    "        del new_vectors[current_index]\n",
    "        for student_b, text_vector_b in new_vectors:\n",
    "            sim_score = similarity(text_vector_a, text_vector_b)[0][1]\n",
    "            student_pair = sorted((student_a, student_b))\n",
    "            score = (student_pair[0], student_pair[1], sim_score)\n",
    "            plagiarism_results.add(score)\n",
    "    return plagiarism_results\n",
    "\n",
    "for data in check_plagiarism():\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaff9dbb-c8d6-47ec-95c2-08b8bcc88ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tương đồng: 0.12905569378720508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def vectorize(Text): \n",
    "    return TfidfVectorizer().fit_transform(Text).toarray()\n",
    "\n",
    "def similarity(doc1, doc2): \n",
    "    return cosine_similarity([doc1, doc2])\n",
    "\n",
    "# Đọc nội dung từ hai tệp tin văn bản\n",
    "document1_text = open(\"doan_7.txt\", encoding='utf-8').read()\n",
    "document2_text = open(\"doan_8.txt\", encoding='utf-8').read()\n",
    "\n",
    "# Chuyển đổi đoạn văn thành vector\n",
    "vectors = vectorize([document1_text, document2_text])\n",
    "\n",
    "# So sánh tương đồng giữa hai đoạn văn\n",
    "similarity_score = similarity(vectors[0], vectors[1])[0][1]\n",
    "\n",
    "print(\"Tương đồng:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a399da3-f2d3-4494-9b65-e34a6702bbf1",
   "metadata": {},
   "source": [
    "# Test với url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed68d7d-c203-4fbe-8d19-3f38a6b0e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from underthesea import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL của trang web\n",
    "url = \"https://vnexpress.net/\"\n",
    "\n",
    "# Sử dụng requests để tải nội dung trang web\n",
    "response = requests.get(url)\n",
    "\n",
    "# Kiểm tra xem việc tải nội dung thành công hay không\n",
    "if response.status_code == 200:\n",
    "    # Lấy nội dung HTML của trang web\n",
    "    html = response.text\n",
    "\n",
    "    # Tạo một đối tượng BeautifulSoup để phân tích nội dung HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Trích xuất văn bản từ trang web và loại bỏ các đoạn trống\n",
    "    text = '\\n'.join([line.strip() for line in soup.stripped_strings])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abfbc30-1478-4b53-a271-870cdd81c2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Ignoring args : ('The rapid development of artificial intelligence (AI) has become a defining feature of our technological landscape. AI applications, ranging from virtual assistants to self-driving cars, have become an integral part of our daily lives. Machine learning, a key component of AI, empowers computers to learn and make decisions based on data, contributing to the advancement of various fields.\\n\\nNatural language processing (NLP) is an area where AI has shown remarkable progress. NLP algorithms enable machines to understand, interpret, and generate human-like language, facilitating improvements in language translation, virtual assistants, and sentiment analysis.\\n\\nWhile AI brings numerous advantages, it also raises ethical concerns and questions about potential job displacement. Responsible and fair use of AI technologies requires a collaborative effort among technologists, policymakers, and ethicists. Addressing these ethical challenges is essential for ensuring a positive and sustainable future as AI continues to evolve.\\n\\nIn summary, the transformative impact of artificial intelligence on our technological landscape is undeniable. As we navigate this dynamic landscape, prioritizing ethical considerations is crucial to harness the full potential of AI for the benefit of humanity.\\n',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tương đồng TF-IDF: 0.7479716320477551\n",
      "Tương đồng DistilBERT: 0.4788822531700134\n",
      "Is Plagiarized: False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, pipeline\n",
    "\n",
    "def vectorize(Text): \n",
    "    return TfidfVectorizer().fit_transform(Text).toarray()\n",
    "\n",
    "def similarity(doc1, doc2): \n",
    "    return cosine_similarity([doc1, doc2])\n",
    "\n",
    "def calculate_similarity_score_bert(text1, text2):\n",
    "    # Sử dụng DistilBERT để tính toán độ tương đồng giữa hai đoạn văn\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=1)\n",
    "    similarity_analyzer = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Tính toán độ tương đồng\n",
    "    similarity_score = similarity_analyzer(text1, text2)[0]['score']\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "def check_plagiarism_with_tfidf_bert(text1, text2, threshold_tfidf=0.8, threshold_bert=0.8):\n",
    "    # Vector hóa dữ liệu bằng TF-IDF\n",
    "    vectors_tfidf = vectorize([text1, text2])\n",
    "\n",
    "    # Tính toán độ tương đồng với TF-IDF\n",
    "    similarity_score_tfidf = similarity(vectors_tfidf[0], vectors_tfidf[1])[0][1]\n",
    "\n",
    "    # Tính toán độ tương đồng với DistilBERT\n",
    "    similarity_score_bert = calculate_similarity_score_bert(text1, text2)\n",
    "\n",
    "    print(\"Tương đồng TF-IDF:\", similarity_score_tfidf)\n",
    "    print(\"Tương đồng DistilBERT:\", similarity_score_bert)\n",
    "\n",
    "    # Kết hợp kết quả từ cả hai phương pháp\n",
    "    combined_score = (similarity_score_tfidf + similarity_score_bert) / 2\n",
    "\n",
    "    return combined_score >= max(threshold_tfidf, threshold_bert)\n",
    "\n",
    "# Đọc nội dung từ hai tệp tin văn bản\n",
    "document1_text = open(\"doan_1.txt\", encoding='utf-8').read()\n",
    "document2_text = open(\"doan_2.txt\", encoding='utf-8').read()\n",
    "\n",
    "# Kiểm tra đạo văn\n",
    "is_plagiarized = check_plagiarism_with_tfidf_bert(document1_text, document2_text)\n",
    "print(\"Is Plagiarized:\", is_plagiarized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ffa7c-0b7b-4028-adfa-63daaeadf24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
